{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea9c991-3f8c-4b47-a614-0524979cdbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow==2.13.2 in /opt/conda/lib/python3.10/site-packages (2.13.2)\n",
      "Requirement already satisfied: sagemaker-mlflow==0.1.0 in /opt/conda/lib/python3.10/site-packages (0.1.0)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.13.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (2.2.1)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (7.0.0)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (6.10.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.8.4)\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.26.4)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.25.0)\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (23.2)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (2.1.4)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (4.24.4)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (15.0.0)\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (2023.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (6.0.1)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.2.4)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.4.2)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.11.4)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (1.4.49)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (0.4.4)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<23 in /opt/conda/lib/python3.10/site-packages (from mlflow==2.13.2) (22.0.0)\n",
      "Requirement already satisfied: boto3>=1.34 in /opt/conda/lib/python3.10/site-packages (from sagemaker-mlflow==0.1.0) (1.34.51)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.13.2) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.13.2) (4.11.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.34->sagemaker-mlflow==0.1.0) (0.10.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow==2.13.2) (1.26.18)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.13.2) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.13.2) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow==2.13.2) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow==2.13.2) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow==2.13.2) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow==2.13.2) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow==2.13.2) (9.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.13.2) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.13.2) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.13.2) (2.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.0.0->mlflow==2.13.2) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.0.0->mlflow==2.13.2) (0.46b0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow==2.13.2) (2024.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from querystring-parser<2->mlflow==2.13.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.13.2) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.13.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.13.2) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.13.2) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.0.0->mlflow==2.13.2) (1.14.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow==2.13.2) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow==2.13.2 sagemaker-mlflow==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d033d7b-3898-4365-a240-741644661e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import requests\n",
    "import torch\n",
    "import tqdm\n",
    "import mlflow\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d9f78c-2220-427f-8f60-18395104a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "sm_client = boto_session.client(\"sagemaker\")\n",
    "sm_role = sagemaker.get_execution_role()\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define your feature group name and region\n",
    "feature_group_name = 'fire-image-feature-group'\n",
    "\n",
    "# Athena client\n",
    "athena_client = boto3.client('athena', region_name=region)\n",
    "\n",
    "# MLFLow\n",
    "tracking_server_arn = 'arn:aws:sagemaker:eu-central-1:567821811420:mlflow-tracking-server/wildfire-mj'\n",
    "experiment_name = 'wildfire-team2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee236211-5995-44bf-ba44-aef5c64fb10d",
   "metadata": {},
   "source": [
    "**Taking Data from Feature Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9b9052-fde9-4a73-bfcc-3039dc4e599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature group\n",
    "feature_group = FeatureGroup(name=feature_group_name, sagemaker_session=sm_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcee432-bafc-409a-b90c-160a01c5fa01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'VarCharValue': '3fcaee94-dd1c-4350-ace3-bbe7fb57b7aa'},\n",
       "  {'VarCharValue': 's3://wildfires/fire_images/fire.1.png'},\n",
       "  {'VarCharValue': '1'},\n",
       "  {'VarCharValue': 'png'},\n",
       "  {'VarCharValue': '2024-06-18T07:28:48.589045Z'},\n",
       "  {'VarCharValue': '2024-06-18 07:33:47.967'},\n",
       "  {'VarCharValue': '2024-06-18 07:28:53.000'},\n",
       "  {'VarCharValue': 'false'}],\n",
       " [{'VarCharValue': 'ece53839-d424-4234-8dbd-cb9e7353fa16'},\n",
       "  {'VarCharValue': 's3://wildfires/fire_images/fire.408.png'},\n",
       "  {'VarCharValue': '1'},\n",
       "  {'VarCharValue': 'png'},\n",
       "  {'VarCharValue': '2024-06-18T07:28:48.592019Z'},\n",
       "  {'VarCharValue': '2024-06-18 07:33:47.967'},\n",
       "  {'VarCharValue': '2024-06-18 07:28:53.000'},\n",
       "  {'VarCharValue': 'false'}],\n",
       " [{'VarCharValue': 'e367966d-3b98-4cd1-a78a-274f8d889f13'},\n",
       "  {'VarCharValue': 's3://wildfires/fire_images/fire.107.png'},\n",
       "  {'VarCharValue': '1'},\n",
       "  {'VarCharValue': 'png'},\n",
       "  {'VarCharValue': '2024-06-18T07:28:48.589121Z'},\n",
       "  {'VarCharValue': '2024-06-18 07:33:47.967'},\n",
       "  {'VarCharValue': '2024-06-18 07:28:53.000'},\n",
       "  {'VarCharValue': 'false'}],\n",
       " [{'VarCharValue': 'e5245fa4-3f2d-49f3-8274-24ea895ca0ab'},\n",
       "  {'VarCharValue': 's3://wildfires/fire_images/fire.410.png'},\n",
       "  {'VarCharValue': '1'},\n",
       "  {'VarCharValue': 'png'},\n",
       "  {'VarCharValue': '2024-06-18T07:28:48.592048Z'},\n",
       "  {'VarCharValue': '2024-06-18 07:33:47.967'},\n",
       "  {'VarCharValue': '2024-06-18 07:28:53.000'},\n",
       "  {'VarCharValue': 'false'}],\n",
       " [{'VarCharValue': '323702c1-5bcd-4a85-aae0-51f6f62459e8'},\n",
       "  {'VarCharValue': 's3://wildfires/fire_images/fire.412.png'},\n",
       "  {'VarCharValue': '1'},\n",
       "  {'VarCharValue': 'png'},\n",
       "  {'VarCharValue': '2024-06-18T07:28:48.592062Z'},\n",
       "  {'VarCharValue': '2024-06-18 07:33:47.967'},\n",
       "  {'VarCharValue': '2024-06-18 07:28:53.000'},\n",
       "  {'VarCharValue': 'false'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query\n",
    "query = \"\"\"SELECT *\n",
    "FROM \"AwsDataCatalog\".\"sagemaker_featurestore\".\"fire_image_feature_group_1718694943\";\n",
    "\"\"\"\n",
    "\n",
    "# Run query\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=query,\n",
    "    QueryExecutionContext={\n",
    "        'Database': 'sagemaker_featurestore'  # Replace with your Athena database name\n",
    "    },\n",
    "    ResultConfiguration={\n",
    "        'OutputLocation': 's3://wildfires/feature-store-output/'  # Replace with your S3 bucket\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get query execution ID\n",
    "query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "# Wait for the query to complete\n",
    "status = 'RUNNING'\n",
    "while status != 'SUCCEEDED':\n",
    "    response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "    status = response['QueryExecution']['Status']['State']\n",
    "\n",
    "# Get the results\n",
    "response = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "# Process the results into a DataFrame\n",
    "rows = [row['Data'] for row in response['ResultSet']['Rows'][1:]]\n",
    "columns = [col['VarCharValue'] for col in response['ResultSet']['Rows'][0]['Data']]\n",
    "\n",
    "rows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a51a35-04cf-422d-99a1-25d49eb5bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "def download_images(metadata, download_dir='images'):\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    for record in metadata:\n",
    "        image_location = record['image_location']\n",
    "        bucket, key = image_location.replace('s3://', '').split('/', 1)\n",
    "        local_path = os.path.join(download_dir, os.path.basename(key))\n",
    "\n",
    "        s3_client.download_file(bucket, key, local_path)\n",
    "\n",
    "        record['local_path'] = local_path  # Add the local path to the record\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "metadata = [\n",
    "    {\n",
    "        'image_id': row[0]['VarCharValue'],\n",
    "        'image_location': row[1]['VarCharValue'],\n",
    "        'label': int(row[2]['VarCharValue']),\n",
    "        'image_type': row[3]['VarCharValue'],\n",
    "        'event_time': row[4]['VarCharValue'],\n",
    "    } for row in rows\n",
    "]\n",
    "\n",
    "metadata = download_images(metadata)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1638d68c-9af2-4828-8041-da3567b5bf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 599\n",
      "Validation samples: 200\n",
      "Test samples: 200\n"
     ]
    }
   ],
   "source": [
    "# Split the metadata into train, validation, and test sets\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.2, stratify=[m['label'] for m in metadata], random_state=42)\n",
    "train_metadata, val_metadata = train_test_split(train_metadata, test_size=0.25, stratify=[m['label'] for m in train_metadata], random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_metadata)}\")\n",
    "print(f\"Validation samples: {len(val_metadata)}\")\n",
    "print(f\"Test samples: {len(test_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cdedcc9-246b-4987-b5ca-4bb5499ef5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDataset(Dataset):\n",
    "    def __init__(self, metadata, transform=None):\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.metadata[idx]['local_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.metadata[idx]['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4b6c07-2c7c-4ebd-920b-087d68a7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and validation/test datasets\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FireDataset(train_metadata, transform=train_transform)\n",
    "val_dataset = FireDataset(val_metadata, transform=val_test_transform)\n",
    "test_dataset = FireDataset(test_metadata, transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3cb17-cd31-4d28-9327-635a81150eb2",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87594c81-2940-4756-a66d-9fa00f2d0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922749f5-c8f5-464c-a41d-2312531c15df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a473109c-5703-4470-894d-a74549c3dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40d7ac5-37e0-447b-8fda-53c79c0df87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4980dcd6-bf1a-4dce-af39-75bf032c3169",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20073bcf-0a49-4e8f-954f-ffff83c1fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_function, epoch, device, run):\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        train_loss += loss.sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    mlflow.log_metric('training_loss', train_loss, step=epoch, run_id=run.info.run_id)\n",
    "\n",
    "\n",
    "def test(model, test_loader, loss_function, epoch, device, run):\n",
    "    model = model.to(device)\n",
    "    loss_function = loss_function.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_function(output, target).sum().item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    mlflow.log_metric('test_loss', test_loss, step=epoch, run_id=run.info.run_id)\n",
    "    mlflow.log_metric('test_accuracy',\n",
    "                      (correct / len(test_loader.dataset)),\n",
    "                      step=epoch, run_id=run.info.run_id)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282fe26f-753d-4da5-8c9f-958c5b776ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, optimizer, train_loader, test_loader, device, run, n_epochs=1):\n",
    "    for epoch in range(0, n_epochs):\n",
    "        train(model, train_loader, optimizer, criterion, epoch, device, run)\n",
    "        test(model, test_loader, criterion, epoch, device, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac5b32e-6fed-4dee-9ab1-79acd9a195b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/599 (0%)]\tLoss: 0.758472\n",
      "\n",
      "Test set: Average loss: 617.8516, Accuracy: 151/200 (76%)\n",
      "\n",
      "Train Epoch: 1 [0/599 (0%)]\tLoss: 0.223021\n",
      "\n",
      "Test set: Average loss: 0.0167, Accuracy: 179/200 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/599 (0%)]\tLoss: 0.170253\n",
      "\n",
      "Test set: Average loss: 0.0098, Accuracy: 175/200 (88%)\n",
      "\n",
      "Train Epoch: 3 [0/599 (0%)]\tLoss: 0.185246\n",
      "\n",
      "Test set: Average loss: 0.0028, Accuracy: 193/200 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/599 (0%)]\tLoss: 0.179742\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 187/200 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model_filename = \"model_resnet18_v2\"\n",
    "model_folder = \"models\"\n",
    "\n",
    "%mkdir -p models\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=sagemaker.utils.name_from_base(\"classification-wildfire\")) as run:\n",
    "    mlflow.log_params({\n",
    "        \"Training samples\": len(train_metadata),\n",
    "        \"Validation samples\": len(val_metadata),\n",
    "        \"Test samples\": len(test_metadata),\n",
    "        \"Epochs\": EPOCHS,\n",
    "    }, run_id=run.info.run_id)\n",
    "\n",
    "    train_test(model=model,\n",
    "               optimizer=optimizer,\n",
    "               train_loader=train_loader,\n",
    "               test_loader=test_loader,\n",
    "               device=device,\n",
    "               run=run,\n",
    "               n_epochs=EPOCHS)\n",
    "\n",
    "    torch.save(model.state_dict(), f'{model_folder}/{model_filename}.pth')\n",
    "    mlflow.log_artifact(os.path.curdir, f'{model_folder}/{model_filename}.pth', run_id=run.info.run_id)\n",
    "\n",
    "    mlflow.end_run(status='FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67f0a7d6-a975-4cb7-a5d6-f2b988a66e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"model_resnet18\"\n",
    "model_folder = \"models\"\n",
    "\n",
    "%mkdir -p models\n",
    "\n",
    "torch.save(model.state_dict(), f'{model_folder}/{model_filename}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff360ce2-12f2-422a-b197-8405c71c35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# model.save_model(f\"../{model_folder}/{model_filename}\")\n",
    "\n",
    "with tarfile.open(f\"{model_folder}/{model_filename}.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(f'{model_folder}/{model_filename}.pth', arcname=model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78a26e4-0a5f-4be3-94bc-3da7a1961ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "BUCKET_NAME = 'wildfires'\n",
    "\n",
    "s3.upload_file(\n",
    "    f'{model_folder}/{model_filename}.tar.gz',\n",
    "    BUCKET_NAME,\n",
    "    f\"models/{model_filename}.tar.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a16d80-f4cb-40a4-919e-13dc96d7a3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a6843-bff1-4650-a447-1dce9c2746f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a2cd132-5d00-4f71-831a-470f71311e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed5ddb82-113a-4e52-9b3d-ad0d562f5b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b16d6022-b3bd-4923-a891-1c834ed0a230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9718,  2.0293]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7c2d6-3db1-4864-91f2-89f20993d177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2711f805-82fa-429a-a200-8fbdccb58f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0.post304'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfecc817-b5c1-48dc-a806-38730a05919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to resnet-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_filename = \"resnet18_WeWillDeployCorrectlyThisTime\"\n",
    "# Define the S3 bucket and model file path\n",
    "s3_bucket = 'wildfires'\n",
    "model_key = f\"models/{model_filename}.tar.gz\"\n",
    "local_model_path = 'resnet-model.tar.gz'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download the model file from S3\n",
    "s3.download_file(s3_bucket, model_key, local_model_path)\n",
    "\n",
    "print(f'Model downloaded to {local_model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8db1c5a5-d9d0-4072-a62f-d32ef2a8c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['model_resnet18.pth', 'code', 'code/inference.py', 'code/.ipynb_checkpoints', 'code/.ipynb_checkpoints/inference-checkpoint.py']\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open(local_model_path) as tar:\n",
    "    tar.extractall()\n",
    "    extracted_files = tar.getnames()\n",
    "    print(f'Extracted files: {extracted_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50331f71-d351-484d-8eda-8f9af1ae011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_model_path = 'model_resnet18.pth'  # Adjust this path based on the extracted files\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.load_state_dict(torch.load(extracted_model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c6555cab-c665-4fa2-8100-67433f782ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56de3b02-9305-4a8f-828f-34cf411f48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(img_path, transform):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "image_path = 'test_data_1.png'\n",
    "preprocess_data = preprocess_image(image_path, val_test_transform)\n",
    "\n",
    "input_list = input_data.tolist()\n",
    "input_dict = {\"inputs\": input_list}\n",
    "\n",
    "# Serialize the dictionary to a JSON string\n",
    "input_json = json.dumps(input_dict)\n",
    "\n",
    "data = json.loads(input_json)\n",
    "input_data = torch.tensor(data['inputs'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2154efff-b3eb-4b9b-bdef-c2c4069c79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "         [[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]]]])\n"
     ]
    }
   ],
   "source": [
    "print(input_data == preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "360d08b8-2ecb-4887-a8e2-71403475e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be98c367-3d02-4c9f-9543-881788662f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9718,  2.0293]], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    pred = model(input_data)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d731af6-2c4d-4125-a22d-1349f25d5067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc669bf-9899-4b93-8b27-a5591f3f7bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87d119-b9a5-4052-b3ec-158b90c32c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
